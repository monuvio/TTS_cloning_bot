{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TTScloning_bot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdnUOIETjYPc",
        "colab_type": "text"
      },
      "source": [
        "# Bot "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSkq0NE9Nf_f",
        "colab_type": "code",
        "outputId": "17bf4eaa-deb1-4158-80c3-b1e6eb96260f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQwMvrbhUJXR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "0834880e-131b-404b-d19c-c511fc2e3eb3"
      },
      "source": [
        "!pip install python-telegram-bot"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-telegram-bot\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/6c/47932a4041ee76650ad1f45a80e1422077e1e99c08a4d7a61cfbe5393d41/python_telegram_bot-11.1.0-py2.py3-none-any.whl (326kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from python-telegram-bot) (0.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from python-telegram-bot) (2019.6.16)\n",
            "Collecting cryptography (from python-telegram-bot)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/18/c6557f63a6abde34707196fb2cad1c6dc0dbff25a200d5044922496668a4/cryptography-2.7-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 35.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from cryptography->python-telegram-bot) (1.12.0)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography->python-telegram-bot) (1.12.3)\n",
            "Collecting asn1crypto>=0.21.0 (from cryptography->python-telegram-bot)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/cd/35485615f45f30a510576f1a56d1e0a7ad7bd8ab5ed7cdc600ef7cd06222/asn1crypto-0.24.0-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 26.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography->python-telegram-bot) (2.19)\n",
            "Installing collected packages: asn1crypto, cryptography, python-telegram-bot\n",
            "Successfully installed asn1crypto-0.24.0 cryptography-2.7 python-telegram-bot-11.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKBJux7NanMn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "6eb43546-6428-45aa-e295-dbb7619502bb"
      },
      "source": [
        "!git clone https://github.com/CorentinJ/Real-Time-Voice-Cloning.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Real-Time-Voice-Cloning'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 2325 (delta 0), reused 0 (delta 0), pack-reused 2322\n",
            "Receiving objects: 100% (2325/2325), 360.24 MiB | 13.05 MiB/s, done.\n",
            "Resolving deltas: 100% (1274/1274), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9d9HLqAYG2u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b82bfc1-720f-4016-bb6e-82d7518ede0d"
      },
      "source": [
        "cd Real-Time-Voice-Cloning/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Real-Time-Voice-Cloning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvVactAJan6f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "914a6a12-1e9b-4136-ceab-3c2abcd9ccf3"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu<=1.14.0,>=1.10.0 (from -r requirements.txt (line 1))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 66kB/s \n",
            "\u001b[?25hRequirement already satisfied: umap-learn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.3.9)\n",
            "Collecting visdom (from -r requirements.txt (line 3))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/c4/5f5356fd57ae3c269e0e31601ea6487e0622fedc6756a591e4a5fd66cc7a/visdom-0.1.8.8.tar.gz (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 30.9MB/s \n",
            "\u001b[?25hCollecting webrtcvad (from -r requirements.txt (line 4))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/34/e2de2d97f3288512b9ea56f92e7452f8207eb5a0096500badf9dfd48f5e6/webrtcvad-2.0.10.tar.gz (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 22.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: librosa>=0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.6.3)\n",
            "Requirement already satisfied: matplotlib>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (3.0.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (1.16.4)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (4.28.1)\n",
            "Collecting sounddevice (from -r requirements.txt (line 10))\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/f2/7cb487ac7171dfade8af7a368bd8806ecff82016f4ac9894835cd7de9ecc/sounddevice-0.3.13-py2.py3-none-any.whl\n",
            "Collecting Unidecode (from -r requirements.txt (line 11))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 40.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: inflect in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (2.1.0)\n",
            "Collecting PyQt5 (from -r requirements.txt (line 13))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/bd/8a0c863802449f35ad9ca21a1b73190639c206758b4b5e2425617fc99ce9/PyQt5-5.13.0-5.13.0-cp35.cp36.cp37.cp38-abi3-manylinux1_x86_64.whl (62.1MB)\n",
            "\u001b[K     |████████████████████████████████| 62.1MB 50.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 14)) (0.70.8)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 15)) (0.40.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (1.14.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (1.11.2)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (1.14.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (0.1.7)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (0.8.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (0.33.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (1.0.8)\n",
            "Requirement already satisfied: scikit-learn>=0.16 in /usr/local/lib/python3.6/dist-packages (from umap-learn->-r requirements.txt (line 2)) (0.21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from visdom->-r requirements.txt (line 3)) (2.21.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from visdom->-r requirements.txt (line 3)) (4.5.3)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from visdom->-r requirements.txt (line 3)) (17.0.0)\n",
            "Collecting torchfile (from visdom->-r requirements.txt (line 3))\n",
            "  Downloading https://files.pythonhosted.org/packages/91/af/5b305f86f2d218091af657ddb53f984ecbd9518ca9fe8ef4103a007252c9/torchfile-0.1.0.tar.gz\n",
            "Collecting websocket-client (from visdom->-r requirements.txt (line 3))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 38.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from visdom->-r requirements.txt (line 3)) (4.3.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.5.1->-r requirements.txt (line 5)) (2.1.8)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.5.1->-r requirements.txt (line 5)) (4.4.0)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.5.1->-r requirements.txt (line 5)) (0.2.1)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.5.1->-r requirements.txt (line 5)) (0.13.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.2->-r requirements.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.2->-r requirements.txt (line 6)) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.2->-r requirements.txt (line 6)) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.2->-r requirements.txt (line 6)) (2.4.2)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.6/dist-packages (from sounddevice->-r requirements.txt (line 10)) (1.12.3)\n",
            "Collecting PyQt5_sip<13,>=4.19.14 (from PyQt5->-r requirements.txt (line 13))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/15/055d5667166b1d8c04ad159216be7fa254897fafd79e749bc263cb86f802/PyQt5_sip-4.19.18-cp36-cp36m-manylinux1_x86_64.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 24.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from multiprocess->-r requirements.txt (line 14)) (0.3.0)\n",
            "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->-r requirements.txt (line 15)) (0.29.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (41.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu<=1.14.0,>=1.10.0->-r requirements.txt (line 1)) (2.8.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->-r requirements.txt (line 3)) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->visdom->-r requirements.txt (line 3)) (0.46)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from CFFI>=1.0->sounddevice->-r requirements.txt (line 10)) (2.19)\n",
            "Building wheels for collected packages: visdom, webrtcvad, torchfile\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.1.8.8-cp36-none-any.whl size=1350602 sha256=46536a20a4bb364b286e41bff61a25a41854f76d3c4093fec18441c88f940522\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/87/ce/a5023722374ca73b57fc8d4284ba6f973c01219b3c385a07e0\n",
            "  Building wheel for webrtcvad (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for webrtcvad: filename=webrtcvad-2.0.10-cp36-cp36m-linux_x86_64.whl size=71348 sha256=54f1186210a36636c10edb51ba58ec165f775d950408b687bd2ba412ebebbf48\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/2a/18/bd1aec41cac7c3051fe95d92a6ed446122ea31dc713c432fa1\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-cp36-none-any.whl size=5713 sha256=8d1c0c13abdfb2b726e92f5a9bfa754579c43b699b835e4c2b6b0b82ff83f88a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/c3/d6/9a1cc8f3a99a0fc1124cae20153f36af59a6e683daca0a0814\n",
            "Successfully built visdom webrtcvad torchfile\n",
            "Installing collected packages: tensorflow-gpu, torchfile, websocket-client, visdom, webrtcvad, sounddevice, Unidecode, PyQt5-sip, PyQt5\n",
            "Successfully installed PyQt5-5.13.0 PyQt5-sip-4.19.18 Unidecode-1.1.1 sounddevice-0.3.13 tensorflow-gpu-1.14.0 torchfile-0.1.0 visdom-0.1.8.8 webrtcvad-2.0.10 websocket-client-0.56.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY6C1wLParGn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "640e84bf-82cd-42b6-e2db-52aecc87fd67"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1n1sPXvT34yXFLT47QZA6FIRGrwMeSsZc"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1n1sPXvT34yXFLT47QZA6FIRGrwMeSsZc\n",
            "To: /content/Real-Time-Voice-Cloning/pretrained.zip\n",
            "384MB [00:05, 64.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-kkWdomasi_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "c1698aab-5580-48c0-9bc9-dc45f3dec60f"
      },
      "source": [
        "!unzip pretrained.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  pretrained.zip\n",
            "   creating: encoder/saved_models/\n",
            "  inflating: encoder/saved_models/pretrained.pt  \n",
            "   creating: synthesizer/saved_models/\n",
            "   creating: synthesizer/saved_models/logs-pretrained/\n",
            "   creating: synthesizer/saved_models/logs-pretrained/taco_pretrained/\n",
            " extracting: synthesizer/saved_models/logs-pretrained/taco_pretrained/checkpoint  \n",
            "  inflating: synthesizer/saved_models/logs-pretrained/taco_pretrained/tacotron_model.ckpt-278000.data-00000-of-00001  \n",
            "  inflating: synthesizer/saved_models/logs-pretrained/taco_pretrained/tacotron_model.ckpt-278000.index  \n",
            "  inflating: synthesizer/saved_models/logs-pretrained/taco_pretrained/tacotron_model.ckpt-278000.meta  \n",
            "   creating: vocoder/saved_models/\n",
            "   creating: vocoder/saved_models/pretrained/\n",
            "  inflating: vocoder/saved_models/pretrained/pretrained.pt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOuqstk3au78",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "7693d74e-5dae-4f51-fd74-3e3389654bc8"
      },
      "source": [
        "!apt-get install libportaudio2"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2\n",
            "0 upgraded, 1 newly installed, 0 to remove and 4 not upgraded.\n",
            "Need to get 64.6 kB of archives.\n",
            "After this operation, 215 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libportaudio2 amd64 19.6.0-1 [64.6 kB]\n",
            "Fetched 64.6 kB in 1s (46.1 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 131289 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlZ59EsPNnwX",
        "colab_type": "code",
        "outputId": "936f184b-679b-4dbb-b104-7e6d6f88dfa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from telegram.ext import Updater, CommandHandler, MessageHandler, RegexHandler\n",
        "from telegram.ext import ConversationHandler, CallbackQueryHandler, Filters\n",
        "from telegram import InlineKeyboardButton, InlineKeyboardMarkup, InlineQueryResultVoice\n",
        "from telegram import ReplyKeyboardMarkup, ReplyKeyboardRemove\n",
        "from encoder.params_model import model_embedding_size as speaker_embedding_size\n",
        "from utils.argutils import print_args\n",
        "from synthesizer.inference import Synthesizer\n",
        "from encoder import inference as encoder\n",
        "from vocoder import inference as vocoder\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import librosa\n",
        "import argparse\n",
        "import torch\n",
        "import sys\n",
        "#from telegram_token import token\n",
        "import logging\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "\n",
        "from array import array\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import io\n",
        "import subprocess\n",
        "\n",
        "#from model_1 import transfer_style\n",
        "#from model_2 import transform\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "logging.basicConfig(\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    level=logging.INFO)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "MENU, SET_STAT, ABOUT, MODEL, TTS, PREP = range(6)\n",
        "\n",
        "\n",
        "def start(bot, update):\n",
        "    \"\"\"\n",
        "    Start function. Calls when the /start command is called.\n",
        "    \"\"\"\n",
        "    return menu(bot, update)\n",
        "\n",
        "\n",
        "def menu(bot, update):\n",
        "    \"\"\"\n",
        "    Main menu function.\n",
        "    This will display the options from the main menu.\n",
        "    \"\"\"\n",
        "    keyboard = [[\"Клонирование голоса\", \"О боте\"]]\n",
        "    reply_markup = ReplyKeyboardMarkup(keyboard,\n",
        "                                       one_time_keyboard=True,\n",
        "                                       resize_keyboard=True)\n",
        "    user = update.message.from_user\n",
        "    logger.info(\"Menu command shows for {}.\".format(user.first_name))\n",
        "    update.message.reply_text(\"Привет! Я бот, который может скопировать твою речь! \\n\"\\\n",
        "                                \"Выбирай одну из функций ниже\", reply_markup=reply_markup)\n",
        "    return SET_STAT\n",
        "\n",
        "\n",
        "def set_state(bot, update):\n",
        "    \"\"\"\n",
        "    Set option selected from menu.\n",
        "    \"\"\"\n",
        "    user = update.message.from_user\n",
        "    if update.message.text == \"Клонирование голоса\":\n",
        "        return redirect_to_prep(bot, update)\n",
        "    elif update.message.text == \"О боте\":\n",
        "        return about_bot(bot, update)\n",
        "    else:\n",
        "        return MENU\n",
        "\n",
        "      \n",
        "def redirect_to_prep(bot,update):\n",
        "    update.message.reply_text(\"Отправь голосовое сообщение, произнеси: 'Six big devils from Japan quickly forgot how to waltz'\")\n",
        "    return PREP\n",
        "\n",
        "def prep_func(bot, update):\n",
        "    \"\"\"\n",
        "    Start\n",
        "    \"\"\"\n",
        "    update.message.reply_text(\"Дай мне несколько секунд, не уходи\")\n",
        "    chat_id = update.message.chat_id\n",
        "    print(\"Got voice message from {}\".format(chat_id))\n",
        "    \n",
        "\n",
        "    file = bot.getFile(update.message.voice.file_id)\n",
        "    file.download('voice.ogg')\n",
        "    \n",
        "    process = subprocess.run(['ffmpeg', '-i', 'voice.ogg', 'for-model.wav'])\n",
        "    if process.returncode != 0:\n",
        "      raise Exception(\"Something went wrong\")\n",
        "\n",
        "    device_id = torch.cuda.current_device()\n",
        "\n",
        "    ## Load the models one by one.\n",
        "    print(\"Preparing the encoder, the synthesizer and the vocoder...\")\n",
        "    encoder.load_model('encoder/saved_models/pretrained.pt')\n",
        "    synthesizer = Synthesizer(\"synthesizer/saved_models/logs-pretrained/taco_pretrained\")\n",
        "    vocoder.load_model('vocoder/saved_models/pretrained/pretrained.pt')\n",
        "    \n",
        "    \n",
        "    print(\"\\tTesting the encoder...\")\n",
        "    encoder.embed_utterance(np.zeros(encoder.sampling_rate))\n",
        "    \n",
        "    # Create a dummy embedding. You would normally use the embedding that encoder.embed_utterance\n",
        "    # returns, but here we're going to make one ourselves just for the sake of showing that it's\n",
        "    # possible.\n",
        "    embed = np.random.rand(speaker_embedding_size)\n",
        "    # Embeddings are L2-normalized (this isn't important here, but if you want to make your own \n",
        "    # embeddings it will be).\n",
        "    embed /= np.linalg.norm(embed)\n",
        "    # The synthesizer can handle multiple inputs with batching. Let's create another embedding to \n",
        "    # illustrate that\n",
        "    embeds = [embed, np.zeros(speaker_embedding_size)]\n",
        "    texts = [\"test 1\", \"test 2\"]\n",
        "    print(\"\\tTesting the synthesizer... (loading the model will output a lot of text)\")\n",
        "    mels = synthesizer.synthesize_spectrograms(texts, embeds)\n",
        "    \n",
        "    # The vocoder synthesizes one waveform at a time, but it's more efficient for long ones. We \n",
        "    # can concatenate the mel spectrograms to a single one.\n",
        "    mel = np.concatenate(mels, axis=1)\n",
        "    # The vocoder can take a callback function to display the generation. More on that later. For \n",
        "    # now we'll simply hide it like this:\n",
        "    #no_action = lambda *args: None\n",
        "\n",
        "    \n",
        "    num_generated = 0\n",
        "    try:\n",
        "        # Get the reference audio filepath\n",
        "        message = \"Reference voice: enter an audio filepath of a voice to be cloned (mp3, \" \\\n",
        "                  \"wav, m4a, flac, ...):\\n\"\n",
        "        in_fpath = 'for-model.wav'\n",
        "        \n",
        "        \n",
        "        ## Computing the embedding\n",
        "        # First, we load the wav using the function that the speaker encoder provides. This is \n",
        "        # important: there is preprocessing that must be applied.\n",
        "        \n",
        "        # The following two methods are equivalent:\n",
        "        # - Directly load from the filepath:\n",
        "        preprocessed_wav = encoder.preprocess_wav(in_fpath)\n",
        "        # - If the wav is already loaded:\n",
        "        original_wav, sampling_rate = librosa.load(in_fpath)\n",
        "        preprocessed_wav = encoder.preprocess_wav(original_wav, sampling_rate)\n",
        "        print(\"Loaded file succesfully\")\n",
        "        \n",
        "        # Then we derive the embedding. There are many functions and parameters that the \n",
        "        # speaker encoder interfaces. These are mostly for in-depth research. You will typically\n",
        "        # only use this function (with its default parameters):\n",
        "        embed = encoder.embed_utterance(preprocessed_wav)\n",
        "        print(\"Created the embedding\")\n",
        "        \n",
        "        \n",
        "        ## Generating the spectrogram\n",
        "        text = 'perfect day for good thing'\n",
        "        \n",
        "        # The synthesizer works in batch, so you need to put your data in a list or numpy array\n",
        "        texts = [text]\n",
        "        embeds = [embed]\n",
        "        # If you know what the attention layer alignments are, you can retrieve them here by\n",
        "        # passing return_alignments=True\n",
        "        specs = synthesizer.synthesize_spectrograms(texts, embeds)\n",
        "        spec = specs[0]\n",
        "        print(\"Created the mel spectrogram\")\n",
        "        \n",
        "        \n",
        "        ## Generating the waveform\n",
        "        print(\"Synthesizing the waveform:\")\n",
        "        # Synthesizing the waveform is fairly straightforward. Remember that the longer the\n",
        "        # spectrogram, the more time-efficient the vocoder.\n",
        "        generated_wav = vocoder.infer_waveform(spec)\n",
        "        \n",
        "        \n",
        "        ## Post-generation\n",
        "        # There's a bug with sounddevice that makes the audio cut one second earlier, so we\n",
        "        # pad it.\n",
        "        generated_wav = np.pad(generated_wav, (0, synthesizer.sample_rate), mode=\"constant\")\n",
        "        \n",
        "        # Play the audio (non-blocking)\n",
        "        # sd.stop()\n",
        "        # sd.play(generated_wav, synthesizer.sample_rate)\n",
        "            \n",
        "        # Save it on the disk\n",
        "        print(generated_wav.dtype)\n",
        "        librosa.output.write_wav('perfect_day_for_good_thing.wav', generated_wav.astype(np.float32), \n",
        "                                 synthesizer.sample_rate)\n",
        "        \n",
        "        \n",
        "    except Exception as e:\n",
        "        print(\"Caught exception: %s\" % repr(e))\n",
        "        print(\"Restarting\\n\")\n",
        "\n",
        "    os.remove('for-model.wav')\n",
        "    os.remove('voice.ogg')\n",
        "\n",
        "    bot.send_audio(chat_id=chat_id, audio=open('perfect_day_for_good_thing.wav', 'rb'))\n",
        "  \n",
        "def synthesize(bot, update):\n",
        "    bot.send_audio(chat_id=chat_id, audio=open('tests/test.mp3', 'rb'))\n",
        "    return MENU\n",
        "\n",
        "def about_bot(bot, update):\n",
        "    \"\"\"\n",
        "    About bot. Displays info about Style Transfer Bot and his creator.\n",
        "    \"\"\"\n",
        "    user = update.message.from_user\n",
        "    logger.info(\"About bot info requested by {}.\".format(user.first_name))\n",
        "    bot.send_message(chat_id=update.message.chat_id, text=\n",
        "    \"\"\"\n",
        "    Сайт бота: ###\n",
        "\t  Github разработчика: ###\n",
        "\t  Сайт разработчика: ###\n",
        "\t\n",
        "    Этот бот создан в рамках проектной работы в Deep learning school.\n",
        "    \"\"\")\n",
        "    bot.send_message(chat_id=update.message.chat_id, text=\"Ты можешь вернуться обратно в меню с помощью команды /menu.\")\n",
        "    return MENU\n",
        "\n",
        "\n",
        "def help(bot, update):\n",
        "    \"\"\"\n",
        "    Help function.\n",
        "    This displays a set of commands available for the bot.\n",
        "    \"\"\"\n",
        "    user = update.message.from_user\n",
        "    logger.info(\"User {} asked for help.\".format(user.first_name))\n",
        "    update.message.reply_text(\n",
        "        \"Используй команду /cancel , чтобы выйти из чата. \\nИспользуй /start , чтобы перезагрузить бота.\",\n",
        "        reply_markup=ReplyKeyboardRemove())\n",
        "\n",
        "\n",
        "def cancel(bot, update):\n",
        "    \"\"\"\n",
        "    User cancelation function.\n",
        "    Cancel conversation by user.\n",
        "    \"\"\"\n",
        "    user = update.message.from_user\n",
        "    logger.info(\"User {} canceled the conversation.\".format(user.first_name))\n",
        "    update.message.reply_text(\"Пока! Надеемся пообщаться с тобой ещё!\",\n",
        "                              reply_markup=ReplyKeyboardRemove())\n",
        "\n",
        "    return ConversationHandler.END\n",
        "\n",
        "\n",
        "def error(bot, update, error):\n",
        "    \"\"\"Log Errors caused by Updates.\"\"\"\n",
        "    logger.warning('Update \"%s\" caused error \"%s\"', update, error)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function.\n",
        "    This function handles the conversation flow by setting\n",
        "    states on each step of the flow. Each state has its own\n",
        "    handler for the interaction with the user.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the EventHandler and pass it your bot's token.\n",
        "    updater = Updater(token, request_kwargs={'proxy_url': 'socks4://54.37.169.182:47625'})\n",
        "\n",
        "    # Get the dispatcher to register handlers:\n",
        "    dp = updater.dispatcher\n",
        "\n",
        "    # Add conversation handler with predefined states:\n",
        "    conv_handler = ConversationHandler(\n",
        "        entry_points=[CommandHandler('start', start)],\n",
        "\n",
        "        states={\n",
        "            MENU: [CommandHandler('menu', menu)],\n",
        "            \n",
        "            PREP: [MessageHandler(Filters.voice, prep_func)],\n",
        "            \n",
        "            TTS: [MessageHandler(Filters.text, synthesize)],\n",
        "\n",
        "            SET_STAT: [RegexHandler(\n",
        "                '^({}|{})$'.format(\n",
        "                \"Клонирование голоса\", \"О боте\"),\n",
        "                set_state)]\n",
        "        },\n",
        "\n",
        "        fallbacks=[CommandHandler('cancel', cancel),\n",
        "                   CommandHandler('menu', menu),\n",
        "                   CommandHandler('help', help)],\n",
        "                   \n",
        "        conversation_timeout = 900.0\n",
        "    )\n",
        "\n",
        "    dp.add_handler(conv_handler)\n",
        "\n",
        "    # Log all errors:\n",
        "    dp.add_error_handler(error)\n",
        "\n",
        "    # Start Style transfer bot:\n",
        "    updater.start_polling()\n",
        "\n",
        "    # Run the bot until the user presses Ctrl-C or the process\n",
        "    # receives SIGINT, SIGTERM or SIGABRT:\n",
        "    updater.idle()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print('ready')\n",
        "    main()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ready\n",
            "Got voice message from 279363416\n",
            "Preparing the encoder, the synthesizer and the vocoder...\n",
            "Building Wave-RNN\n",
            "Trainable Parameters: 4.481M\n",
            "Loading model weights at vocoder/saved_models/pretrained/pretrained.pt\n",
            "Testing your configuration with small inputs.\n",
            "\tTesting the encoder...\n",
            "\tTesting the synthesizer... (loading the model will output a lot of text)\n",
            "Constructing model: Tacotron\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0821 11:05:49.251325 140296016959232 ag_logging.py:145] Entity <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7f991c8c65c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7f991c8c65c0>>: ValueError: Failed to parse source code of <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7f991c8c65c0>>, which Python reported as:\n",
            "    def __call__(self, inputs, state, scope=None):\n",
            "        \"\"\"Runs vanilla LSTM Cell and applies zoneout.\n",
            "        \"\"\"\n",
            "        # Apply vanilla LSTM\n",
            "        output, new_state = self._cell(inputs, state, scope)\n",
            "\n",
            "        if self.state_is_tuple:\n",
            "            (prev_c, prev_h) = state\n",
            "            (new_c, new_h) = new_state\n",
            "        else:\n",
            "            num_proj = self._cell._num_units if self._cell._num_proj is None else \\\n",
            "\t\t\t\tself._cell._num_proj\n",
            "            prev_c = tf.slice(state, [0, 0], [-1, self._cell._num_units])\n",
            "            prev_h = tf.slice(state, [0, self._cell._num_units], [-1, num_proj])\n",
            "            new_c = tf.slice(new_state, [0, 0], [-1, self._cell._num_units])\n",
            "            new_h = tf.slice(new_state, [0, self._cell._num_units], [-1, num_proj])\n",
            "\n",
            "        # Apply zoneout\n",
            "        if self.is_training:\n",
            "            # nn.dropout takes keep_prob (probability to keep activations) not drop_prob (\n",
            "\t\t\t# probability to mask activations)!\n",
            "            c = (1 - self._zoneout_cell) * tf.nn.dropout(new_c - prev_c,\n",
            "                                                         (1 - self._zoneout_cell)) + prev_c\n",
            "            h = (1 - self._zoneout_outputs) * tf.nn.dropout(new_h - prev_h,\n",
            "                                                            (1 - self._zoneout_outputs)) + prev_h\n",
            "\n",
            "        else:\n",
            "            c = (1 - self._zoneout_cell) * new_c + self._zoneout_cell * prev_c\n",
            "            h = (1 - self._zoneout_outputs) * new_h + self._zoneout_outputs * prev_h\n",
            "\n",
            "        new_state = tf.nn.rnn_cell.LSTMStateTuple(c, h) if self.state_is_tuple else tf.concat(1, [c,\n",
            "                                                                                                  h])\n",
            "\n",
            "        return output, new_state\n",
            "\n",
            "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
            "W0821 11:05:49.312015 140296016959232 ag_logging.py:145] Entity <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7f98e261df98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7f98e261df98>>: ValueError: Failed to parse source code of <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7f98e261df98>>, which Python reported as:\n",
            "    def __call__(self, inputs, state, scope=None):\n",
            "        \"\"\"Runs vanilla LSTM Cell and applies zoneout.\n",
            "        \"\"\"\n",
            "        # Apply vanilla LSTM\n",
            "        output, new_state = self._cell(inputs, state, scope)\n",
            "\n",
            "        if self.state_is_tuple:\n",
            "            (prev_c, prev_h) = state\n",
            "            (new_c, new_h) = new_state\n",
            "        else:\n",
            "            num_proj = self._cell._num_units if self._cell._num_proj is None else \\\n",
            "\t\t\t\tself._cell._num_proj\n",
            "            prev_c = tf.slice(state, [0, 0], [-1, self._cell._num_units])\n",
            "            prev_h = tf.slice(state, [0, self._cell._num_units], [-1, num_proj])\n",
            "            new_c = tf.slice(new_state, [0, 0], [-1, self._cell._num_units])\n",
            "            new_h = tf.slice(new_state, [0, self._cell._num_units], [-1, num_proj])\n",
            "\n",
            "        # Apply zoneout\n",
            "        if self.is_training:\n",
            "            # nn.dropout takes keep_prob (probability to keep activations) not drop_prob (\n",
            "\t\t\t# probability to mask activations)!\n",
            "            c = (1 - self._zoneout_cell) * tf.nn.dropout(new_c - prev_c,\n",
            "                                                         (1 - self._zoneout_cell)) + prev_c\n",
            "            h = (1 - self._zoneout_outputs) * tf.nn.dropout(new_h - prev_h,\n",
            "                                                            (1 - self._zoneout_outputs)) + prev_h\n",
            "\n",
            "        else:\n",
            "            c = (1 - self._zoneout_cell) * new_c + self._zoneout_cell * prev_c\n",
            "            h = (1 - self._zoneout_outputs) * new_h + self._zoneout_outputs * prev_h\n",
            "\n",
            "        new_state = tf.nn.rnn_cell.LSTMStateTuple(c, h) if self.state_is_tuple else tf.concat(1, [c,\n",
            "                                                                                                  h])\n",
            "\n",
            "        return output, new_state\n",
            "\n",
            "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Entity <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7f991c8c65c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7f991c8c65c0>>: ValueError: Failed to parse source code of <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7f991c8c65c0>>, which Python reported as:\n",
            "    def __call__(self, inputs, state, scope=None):\n",
            "        \"\"\"Runs vanilla LSTM Cell and applies zoneout.\n",
            "        \"\"\"\n",
            "        # Apply vanilla LSTM\n",
            "        output, new_state = self._cell(inputs, state, scope)\n",
            "\n",
            "        if self.state_is_tuple:\n",
            "            (prev_c, prev_h) = state\n",
            "            (new_c, new_h) = new_state\n",
            "        else:\n",
            "            num_proj = self._cell._num_units if self._cell._num_proj is None else \\\n",
            "\t\t\t\tself._cell._num_proj\n",
            "            prev_c = tf.slice(state, [0, 0], [-1, self._cell._num_units])\n",
            "            prev_h = tf.slice(state, [0, self._cell._num_units], [-1, num_proj])\n",
            "            new_c = tf.slice(new_state, [0, 0], [-1, self._cell._num_units])\n",
            "            new_h = tf.slice(new_state, [0, self._cell._num_units], [-1, num_proj])\n",
            "\n",
            "        # Apply zoneout\n",
            "        if self.is_training:\n",
            "            # nn.dropout takes keep_prob (probability to keep activations) not drop_prob (\n",
            "\t\t\t# probability to mask activations)!\n",
            "            c = (1 - self._zoneout_cell) * tf.nn.dropout(new_c - prev_c,\n",
            "                                                         (1 - self._zoneout_cell)) + prev_c\n",
            "            h = (1 - self._zoneout_outputs) * tf.nn.dropout(new_h - prev_h,\n",
            "                                                            (1 - self._zoneout_outputs)) + prev_h\n",
            "\n",
            "        else:\n",
            "            c = (1 - self._zoneout_cell) * new_c + self._zoneout_cell * prev_c\n",
            "            h = (1 - self._zoneout_outputs) * new_h + self._zoneout_outputs * prev_h\n",
            "\n",
            "        new_state = tf.nn.rnn_cell.LSTMStateTuple(c, h) if self.state_is_tuple else tf.concat(1, [c,\n",
            "                                                                                                  h])\n",
            "\n",
            "        return output, new_state\n",
            "\n",
            "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
            "WARNING: Entity <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7f98e261df98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7f98e261df98>>: ValueError: Failed to parse source code of <bound method ZoneoutLSTMCell.__call__ of <synthesizer.models.modules.ZoneoutLSTMCell object at 0x7f98e261df98>>, which Python reported as:\n",
            "    def __call__(self, inputs, state, scope=None):\n",
            "        \"\"\"Runs vanilla LSTM Cell and applies zoneout.\n",
            "        \"\"\"\n",
            "        # Apply vanilla LSTM\n",
            "        output, new_state = self._cell(inputs, state, scope)\n",
            "\n",
            "        if self.state_is_tuple:\n",
            "            (prev_c, prev_h) = state\n",
            "            (new_c, new_h) = new_state\n",
            "        else:\n",
            "            num_proj = self._cell._num_units if self._cell._num_proj is None else \\\n",
            "\t\t\t\tself._cell._num_proj\n",
            "            prev_c = tf.slice(state, [0, 0], [-1, self._cell._num_units])\n",
            "            prev_h = tf.slice(state, [0, self._cell._num_units], [-1, num_proj])\n",
            "            new_c = tf.slice(new_state, [0, 0], [-1, self._cell._num_units])\n",
            "            new_h = tf.slice(new_state, [0, self._cell._num_units], [-1, num_proj])\n",
            "\n",
            "        # Apply zoneout\n",
            "        if self.is_training:\n",
            "            # nn.dropout takes keep_prob (probability to keep activations) not drop_prob (\n",
            "\t\t\t# probability to mask activations)!\n",
            "            c = (1 - self._zoneout_cell) * tf.nn.dropout(new_c - prev_c,\n",
            "                                                         (1 - self._zoneout_cell)) + prev_c\n",
            "            h = (1 - self._zoneout_outputs) * tf.nn.dropout(new_h - prev_h,\n",
            "                                                            (1 - self._zoneout_outputs)) + prev_h\n",
            "\n",
            "        else:\n",
            "            c = (1 - self._zoneout_cell) * new_c + self._zoneout_cell * prev_c\n",
            "            h = (1 - self._zoneout_outputs) * new_h + self._zoneout_outputs * prev_h\n",
            "\n",
            "        new_state = tf.nn.rnn_cell.LSTMStateTuple(c, h) if self.state_is_tuple else tf.concat(1, [c,\n",
            "                                                                                                  h])\n",
            "\n",
            "        return output, new_state\n",
            "\n",
            "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
            "initialisation done /gpu:0\n",
            "Initialized Tacotron model. Dimensions (? = dynamic shape): \n",
            "  Train mode:               False\n",
            "  Eval mode:                False\n",
            "  GTA mode:                 False\n",
            "  Synthesis mode:           True\n",
            "  Input:                    (?, ?)\n",
            "  device:                   0\n",
            "  embedding:                (?, ?, 512)\n",
            "  enc conv out:             (?, ?, 512)\n",
            "  encoder out (cond):       (?, ?, 768)\n",
            "  decoder out:              (?, ?, 80)\n",
            "  residual out:             (?, ?, 512)\n",
            "  projected residual out:   (?, ?, 80)\n",
            "  mel out:                  (?, ?, 80)\n",
            "  <stop_token> out:         (?, ?)\n",
            "  Tacotron Parameters       28.439 Million.\n",
            "Loading checkpoint: synthesizer/saved_models/logs-pretrained/taco_pretrained/tacotron_model.ckpt-278000\n",
            "Loaded file succesfully\n",
            "Created the embedding\n",
            "Created the mel spectrogram\n",
            "Synthesizing the waveform:\n",
            "{| ████████████████ 95000/96000 | Batch Size: 10 | Gen Rate: 8.3kHz | }float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4C5SqOWUTGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}